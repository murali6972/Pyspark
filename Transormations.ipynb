{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dfd5c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/murali/spark-3.3.2-bin-hadoop3')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"show\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4963b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet/data_files/*.parquet\n"
     ]
    }
   ],
   "source": [
    "path = 'parquet/data_files/*.parquet'\n",
    "print(path)\n",
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1f06348",
   "metadata": {},
   "source": [
    "Show()\n",
    "\n",
    "It always displays 20 rows of data by default.\n",
    "it takes 3 parameters optionally \n",
    "n = numnber of records that you want to show\n",
    "truncate = True / False --Default value is True so all records will get truncated if they are lengthy\n",
    "vertical = True / Fales -- By default it is False. If want to see them in row level insted of as a table we can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76fb874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " registration_dttm | 2016-02-03 01:55:29 \n",
      " id                | 1                   \n",
      " first_name        | Amanda              \n",
      " last_name         | Jordan              \n",
      " email             | ajordan0@com.com    \n",
      " gender            | Female              \n",
      " ip_address        | 1.197.201.2         \n",
      " cc                | 6759521864920116    \n",
      " country           | Indonesia           \n",
      " birthdate         | 3/8/1971            \n",
      " salary            | 49756.53            \n",
      " title             | Internal Auditor    \n",
      " comments          | 1E+02               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1,truncate=False,vertical = True) # by default spark shows only 20 recrods if you don't specify"
   ]
  },
  {
   "cell_type": "raw",
   "id": "311de499",
   "metadata": {},
   "source": [
    "withColumn()\n",
    "pyspark withColumn() function is a transformation function of DataFrame which is used to \n",
    "1 change the value, \n",
    "2 conver the datatype of an existing Column, \n",
    "3 create a new column etc..\n",
    "\n",
    "When you use any transfoermation functions, we are not changing the existing data frame. but we are creating a new dataframe with new transformations. It is the same case withCoumn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4092aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37bc2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+----------------+---------+\n",
      "|registration_dttm  |id |first_name|email           |birthdate|\n",
      "+-------------------+---+----------+----------------+---------+\n",
      "|2016-02-03 01:55:29|1  |Amanda    |ajordan0@com.com|3/8/1971 |\n",
      "|2016-02-03 11:04:03|2  |Albert    |afreeman1@is.gd |1/16/1968|\n",
      "+-------------------+---+----------+----------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['registration_dttm','id','first_name','email','birthdate']].show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6d43145",
   "metadata": {},
   "source": [
    "we are trying to convert birtdate as date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e330004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8088f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn('birthdate', df['birthdate'].cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9635f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[registration_dttm: timestamp, id: int, first_name: string, last_name: string, email: string, gender: string, ip_address: string, cc: string, country: string, birthdate: string, salary: double, title: string, comments: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('salary', df['salary']*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6840be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+------------+-------------------+--------+-------------------+---------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|  country|birthdate|   salary|salaryDouble|              title|comments|           newTitle|UpdatedBy|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+------------+-------------------+--------+-------------------+---------+\n",
      "|2016-02-03 01:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia|     null| 49756.53|    99513.06|   Internal Auditor|   1E+02|   Internal Auditor|   Murali|\n",
      "|2016-02-03 11:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|     null|150280.17|   300560.34|      Accountant IV|        |      Accountant IV|   Murali|\n",
      "|2016-02-02 19:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|   Russia|     null|144972.51|   289945.02|Structural Engineer|        |Structural Engineer|   Murali|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+------------+-------------------+--------+-------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nObserve the fore changes we did below. \\n    - 'salary*2 as salaryDouble'\\n    - newTitle\\n    - birthdate\\n    - UpdatedBy\\nif we do the same withColumn() it is 4 step process and kills performance. In this case it is single stept process\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is best way to select and manipulate multiple columns in a sing step as it's syntex is in sql format and \n",
    "# and it gets executed in a single step and improve the performance too\n",
    "df.selectExpr('registration_dttm','id','first_name','last_name','email','gender',\\\n",
    "              'ip_address','cc','country','cast(birthdate as date) as birthdate',\\\n",
    "              'salary', 'salary*2 as salaryDouble','title','comments',\\\n",
    "             'title as newTitle','\"Murali\" as UpdatedBy').show(3)\n",
    "\n",
    "'''\n",
    "Observe the fore changes we did below. \n",
    "    - 'salary*2 as salaryDouble'\n",
    "    - newTitle\n",
    "    - birthdate\n",
    "    - UpdatedBy\n",
    "if we do the same withColumn() it is 4 step process and kills performance. In this case it is single stept process\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c82dd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method selectExpr in module pyspark.sql.dataframe:\n",
      "\n",
      "selectExpr(*expr: Union[str, List[str]]) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "    \n",
      "    This is a variant of :func:`select` that accepts SQL expressions.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      "    [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84eecb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumnRenamed in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumnRenamed(existing: str, new: str) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "    This is a no-op if schema doesn't contain the given column name.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    existing : str\n",
      "        string, name of the existing column to rename.\n",
      "    new : str\n",
      "        string, new name of the column.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.withColumnRenamed('age', 'age2').collect()\n",
      "    [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumnRenamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e9799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f20039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- New_Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('comments', 'New_Name').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af442dba",
   "metadata": {},
   "source": [
    "# StructType() & SturctField()\n",
    "\n",
    "### PySpark SturctType() & SturctField() classes are used to programatically specify the schema to the Data Frame and \n",
    "### create complex columns like nested sturct, array and map columns\n",
    "\n",
    "### StructType is collection of StructFields\n",
    "\n",
    "### to use this we need to import StuructTypy, SturctField, StringType,IntegerType from pyspark.sql.types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "064b30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType,DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0605c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\\\n",
    "            (1,'murali',2000),\n",
    "            (2, 'geetha',3000),\n",
    "            (3, 'chintu',4000),\n",
    "            (4, 'Chinna',5000),\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3a739ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| _1|    _2|  _3|\n",
      "+---+------+----+\n",
      "|  1|murali|2000|\n",
      "|  2|geetha|3000|\n",
      "|  3|chintu|4000|\n",
      "|  4|Chinna|5000|\n",
      "+---+------+----+\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c5af75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "                    [\n",
    "                        StructField('Id',IntegerType()),\n",
    "                        StructField('name',StringType()),\n",
    "                        StructField('salary',IntegerType())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7922c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = spark.createDataFrame(data,schema=schema)\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18b0a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "                    [\n",
    "                        StructField(name = 'Id',dataType = IntegerType(),nullable=False),\n",
    "                        StructField(name = 'name',dataType = StringType(),nullable=False),\n",
    "                        StructField(name = 'salary',dataType = IntegerType(),nullable=False)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f0dc2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- salary: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_struc = spark.createDataFrame(data,schema = schema)\n",
    "\n",
    "df_new_struc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f216aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- Name: struct (nullable = false)\n",
      " |    |-- FirstName: string (nullable = true)\n",
      " |    |-- LastName: string (nullable = true)\n",
      " |-- Salary: float (nullable = false)\n",
      "\n",
      "+---+--------------------+------+\n",
      "| id|                Name|Salary|\n",
      "+---+--------------------+------+\n",
      "|  1|     {Reddy, pujari}|2000.0|\n",
      "|  2|{Geetha, Tiruvaip...|3000.0|\n",
      "|  3|   {Kishan, Thomala}|4000.0|\n",
      "|  4|     {sasi, Thomala}|5000.0|\n",
      "+---+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, Name: struct<FirstName:string,LastName:string>, Salary: float]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType,DecimalType, FloatType\n",
    "\n",
    "data_new = [\n",
    "    \n",
    "    (1,['Reddy','pujari'],2000.00),\n",
    "    (2,['Geetha','Tiruvaipati'],3000.00),\n",
    "    (3,['Kishan','Thomala'],4000.00),\n",
    "    (4,['sasi','Thomala'],5000.00)\n",
    "]\n",
    "\n",
    "Name_structure = StructType(\n",
    "    [\n",
    "        StructField('FirstName', StringType()),\n",
    "        StructField('LastName', StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "schema_new = StructType(\n",
    "    [\n",
    "    StructField(name='id', dataType=IntegerType(), nullable=False),\n",
    "    StructField(name='Name', dataType=Name_structure,nullable=False),\n",
    "    StructField(name='Salary',dataType=FloatType(),nullable=False)\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "schema_new = StructType([\n",
    "    StructField(name='id', dataType=IntegerType(), nullable=False),\n",
    "    StructField(name='Name', dataType=Name_structure, nullable=False),\n",
    "    StructField(name='Salary', dataType=DecimalType(), nullable=False)\n",
    "])\n",
    "'''\n",
    "\n",
    "df_new = spark.createDataFrame(data_new,schema=schema_new)\n",
    "df_new.printSchema()\n",
    "df_new.show()\n",
    "display(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c051da",
   "metadata": {},
   "source": [
    "# Array type columns in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c6ac47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Salary: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new = [\n",
    "    \n",
    "    (1,['Reddy','pujari'],2000.00),\n",
    "    (2,['Geetha','Tiruvaipati'],3000.00),\n",
    "    (3,['Kishan','Thomala'],4000.00),\n",
    "    (4,['sasi','Thomala'],5000.00)\n",
    "]\n",
    "\n",
    "schema_new = StructType(\n",
    "    [\n",
    "    StructField(name='id', dataType=IntegerType(), nullable=False),\n",
    "    StructField(name = 'name', dataType=ArrayType(StringType())),\n",
    "    StructField(name='Salary',dataType=FloatType(),nullable=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data_new,schema=schema_new)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "93bad7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+\n",
      "| id|                name|Salary|\n",
      "+---+--------------------+------+\n",
      "|  1|     [Reddy, pujari]|2000.0|\n",
      "|  2|[Geetha, Tiruvaip...|3000.0|\n",
      "|  3|   [Kishan, Thomala]|4000.0|\n",
      "|  4|     [sasi, Thomala]|5000.0|\n",
      "+---+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886bd31",
   "metadata": {},
   "source": [
    "# explode() , split(), array() and array_contains() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "430f476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------------+\n",
      "|Id |Name  |Skills                    |\n",
      "+---+------+--------------------------+\n",
      "|1  |Murali|[pySpark, snowflake, AWS] |\n",
      "|2  |Geetha|[Unix, webx, python]      |\n",
      "|3  |kishan|[Data Sciences, ML, Stats]|\n",
      "|4  |Sasi  |[python, AWS, Glue]       |\n",
      "+---+------+--------------------------+\n",
      "\n",
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,'Murali',['pySpark','snowflake','AWS']),\n",
    "    (2,'Geetha',['Unix','webx','python']),\n",
    "    (3,'kishan',['Data Sciences','ML','Stats']),\n",
    "    (4,'Sasi',['python','AWS','Glue'])\n",
    "]\n",
    "\n",
    "schema = ['Id','Name','Skills']\n",
    "\n",
    "skills_df = spark.createDataFrame(data,schema=schema)\n",
    "\n",
    "skills_df.show(truncate=False)\n",
    "skills_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "035c8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "skill_explode = skills_df.withColumn('skill',explode(col('skills')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2b00953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+-------------+\n",
      "| Id|  Name|              Skills|        skill|\n",
      "+---+------+--------------------+-------------+\n",
      "|  1|Murali|[pySpark, snowfla...|      pySpark|\n",
      "|  1|Murali|[pySpark, snowfla...|    snowflake|\n",
      "|  1|Murali|[pySpark, snowfla...|          AWS|\n",
      "|  2|Geetha|[Unix, webx, python]|         Unix|\n",
      "|  2|Geetha|[Unix, webx, python]|         webx|\n",
      "|  2|Geetha|[Unix, webx, python]|       python|\n",
      "|  3|kishan|[Data Sciences, M...|Data Sciences|\n",
      "|  3|kishan|[Data Sciences, M...|           ML|\n",
      "|  3|kishan|[Data Sciences, M...|        Stats|\n",
      "|  4|  Sasi| [python, AWS, Glue]|       python|\n",
      "|  4|  Sasi| [python, AWS, Glue]|          AWS|\n",
      "|  4|  Sasi| [python, AWS, Glue]|         Glue|\n",
      "+---+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skill_explode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5de4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|  name|        skill|\n",
      "+---+------+-------------+\n",
      "|  1|Murali|      pySpark|\n",
      "|  1|Murali|    snowflake|\n",
      "|  1|Murali|          AWS|\n",
      "|  2|Geetha|         Unix|\n",
      "|  2|Geetha|         webx|\n",
      "|  2|Geetha|       python|\n",
      "|  3|kishan|Data Sciences|\n",
      "|  3|kishan|           ML|\n",
      "|  3|kishan|        Stats|\n",
      "|  4|  Sasi|       python|\n",
      "|  4|  Sasi|          AWS|\n",
      "|  4|  Sasi|         Glue|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skills_df.selectExpr('id', 'name', \"explode(skills) as skill\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e4dac",
   "metadata": {},
   "source": [
    "# split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ed517a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------------+\n",
      "|id |name  |skills                |\n",
      "+---+------+----------------------+\n",
      "|1  |Murali|pySpark,snowflake,AWS |\n",
      "|2  |Geetha|Unix,webx,python      |\n",
      "|3  |kishan|Data Sciences,ML,Stats|\n",
      "|4  |Sasi  |python,AWS,Glue       |\n",
      "+---+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,'Murali','pySpark,snowflake,AWS'),\n",
    "    (2,'Geetha','Unix,webx,python'),\n",
    "    (3,'kishan','Data Sciences,ML,Stats'),\n",
    "    (4,'Sasi','python,AWS,Glue')\n",
    "]\n",
    "\n",
    "schema = ['id','name','skills']\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "63a46b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fc1c9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+-------------+\n",
      "| id|  name|              skills|        skill|\n",
      "+---+------+--------------------+-------------+\n",
      "|  1|Murali|[pySpark, snowfla...|      pySpark|\n",
      "|  1|Murali|[pySpark, snowfla...|    snowflake|\n",
      "|  1|Murali|[pySpark, snowfla...|          AWS|\n",
      "|  2|Geetha|[Unix, webx, python]|         Unix|\n",
      "|  2|Geetha|[Unix, webx, python]|         webx|\n",
      "|  2|Geetha|[Unix, webx, python]|       python|\n",
      "|  3|kishan|[Data Sciences, M...|Data Sciences|\n",
      "|  3|kishan|[Data Sciences, M...|           ML|\n",
      "|  3|kishan|[Data Sciences, M...|        Stats|\n",
      "|  4|  Sasi| [python, AWS, Glue]|       python|\n",
      "|  4|  Sasi| [python, AWS, Glue]|          AWS|\n",
      "|  4|  Sasi| [python, AWS, Glue]|         Glue|\n",
      "+---+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn('skills',split('skills',',')).withColumn('skill',explode(col('skills'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ccb9d0",
   "metadata": {},
   "source": [
    "# Array_Contains() is the funciton to be applied on top of an Array to check if it contains a specific value in it\n",
    "# if it contains the value we will get True other wise we will get False if the Arra is null, we see null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c90bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbae41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
