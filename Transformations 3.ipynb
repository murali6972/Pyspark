{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17de71f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 11:59:11 WARN Utils: Your hostname, murali-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/04/04 11:59:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 11:59:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/murali/spark-3.3.2-bin-hadoop3')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Joins').getOrCreate()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc2f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142d0998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "\n",
      "root\n",
      " |-- EMPNO: integer (nullable = true)\n",
      " |-- ENAME: string (nullable = true)\n",
      " |-- JOB: string (nullable = true)\n",
      " |-- MGR: integer (nullable = true)\n",
      " |-- HIREDATE: string (nullable = true)\n",
      " |-- SAL: integer (nullable = true)\n",
      " |-- COMM: integer (nullable = true)\n",
      " |-- DEPTNO: integer (nullable = true)\n",
      "\n",
      "+------+----------+--------+\n",
      "|DEPTNO|     DNAME|LOCATION|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_path = '/home/murali/Desktop/data_files/emp.csv'\n",
    "\n",
    "emp = spark.read.csv(emp_path,header=True, inferSchema=True)\n",
    "emp.show()\n",
    "emp.printSchema()\n",
    "\n",
    "dept_path = '/home/murali/Desktop/data_files/dept.csv'\n",
    "dept = spark.read.csv(dept_path,header=True, inferSchema=True)\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef74666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------------+\n",
      "|DEPTNO|      JOB|Number of Persons|\n",
      "+------+---------+-----------------+\n",
      "|    20|  ANALYST|                2|\n",
      "|    20|  MANAGER|                1|\n",
      "|    30|  MANAGER|                1|\n",
      "|    30| SALESMAN|                4|\n",
      "|    30|    CLERK|                1|\n",
      "|    20|    CLERK|                2|\n",
      "|    10|PRESIDENT|                1|\n",
      "|    10|    CLERK|                1|\n",
      "|    10|  MANAGER|                1|\n",
      "+------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count,min,max,sum\n",
    "empByDeptJob = emp.groupBy('DEPTNO','JOB').agg(                                              \\\n",
    "                               count('EMPNO').alias('Number of Persons')                     \\\n",
    "                              )\n",
    "\n",
    "empByDeptJob.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f911f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_pivote = emp.groupBy('JOB').pivot('DEPTNO').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c173115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+----+\n",
      "|      JOB|  10|  20|  30|\n",
      "+---------+----+----+----+\n",
      "|  ANALYST|null|   2|null|\n",
      "| SALESMAN|null|null|   4|\n",
      "|    CLERK|   1|   2|   1|\n",
      "|  MANAGER|   1|   1|   1|\n",
      "|PRESIDENT|   1|null|null|\n",
      "+---------+----+----+----+\n",
      "\n",
      "+---------+---+---+---+\n",
      "|      JOB| 10| 20| 30|\n",
      "+---------+---+---+---+\n",
      "|  ANALYST|  0|  2|  0|\n",
      "| SALESMAN|  0|  0|  4|\n",
      "|    CLERK|  1|  2|  1|\n",
      "|  MANAGER|  1|  1|  1|\n",
      "|PRESIDENT|  1|  0|  0|\n",
      "+---------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_pivote.show()\n",
    "emp_pivote = emp_pivote.fillna(0)\n",
    "emp_pivote.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816de976",
   "metadata": {},
   "source": [
    "# Unpivote :\n",
    "\n",
    "\n",
    "we don't have unpivote directly. the option is to use stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c7c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47e82b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+----+\n",
      "|      job| d10| d20| d30|\n",
      "+---------+----+----+----+\n",
      "|  ANALYST|null|   2|   0|\n",
      "| SALESMAN|null|null|   4|\n",
      "|    CLERK|   1|   2|   1|\n",
      "|  MANAGER|   1|   1|   1|\n",
      "|PRESIDENT|   1|null|null|\n",
      "+---------+----+----+----+\n",
      "\n",
      "+---------+----------+-----+\n",
      "|      JOB|    gender|count|\n",
      "+---------+----------+-----+\n",
      "|  ANALYST|ACCOUNTING|    0|\n",
      "|  ANALYST|  RESEARCH|    2|\n",
      "|  ANALYST|     SALES|    0|\n",
      "| SALESMAN|ACCOUNTING|    0|\n",
      "| SALESMAN|  RESEARCH|    0|\n",
      "| SALESMAN|     SALES|    4|\n",
      "|    CLERK|ACCOUNTING|    1|\n",
      "|    CLERK|  RESEARCH|    2|\n",
      "|    CLERK|     SALES|    1|\n",
      "|  MANAGER|ACCOUNTING|    1|\n",
      "|  MANAGER|  RESEARCH|    1|\n",
      "|  MANAGER|     SALES|    1|\n",
      "|PRESIDENT|ACCOUNTING|    1|\n",
      "|PRESIDENT|  RESEARCH|    0|\n",
      "|PRESIDENT|     SALES|    0|\n",
      "+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('ANALYST',None, 2,0),\n",
    "    ('SALESMAN',None,None,4),\n",
    "    ('CLERK',1,2,1),\n",
    "    ('MANAGER',1,1,1),\n",
    "    ('PRESIDENT',1,None,None)    \n",
    "]\n",
    "schema = ('job','d10','d20','d30')\n",
    "\n",
    "emp_pivote = spark.createDataFrame(data,schema=schema) \n",
    "emp_pivote.show()\n",
    "emp_pivote = emp_pivote.fillna(0)\n",
    "emp_pivote = emp_pivote.select('JOB',expr(\"stack(3,'ACCOUNTING',d10,'RESEARCH',d20,'SALES',d30) as (gender,count)\"))\n",
    "\n",
    "emp_pivote.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5895f8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|    dep|male|female|\n",
      "+-------+----+------+\n",
      "|     IT|   8|     5|\n",
      "|Payroll|null|     2|\n",
      "|     HR|   2|  null|\n",
      "+-------+----+------+\n",
      "\n",
      "+-------+------+-----+\n",
      "|    dep|gender|count|\n",
      "+-------+------+-----+\n",
      "|     IT|  male|    8|\n",
      "|     IT|female|    5|\n",
      "|Payroll|  male| null|\n",
      "|Payroll|female|    2|\n",
      "|     HR|  male|    2|\n",
      "|     HR|female| null|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        ('IT',8,5),\n",
    "        ('Payroll',None,2),\n",
    "        ('HR',2,None)    \n",
    "]\n",
    "\n",
    "schema = ['dep','male','female']\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "unpivotDF = df.select('dep',expr(\"stack(2,'male',male,'female',female) as (gender,count)\"))\n",
    "unpivotDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e20de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+----+\n",
      "| id|  name|gender| dep|  _5|\n",
      "+---+------+------+----+----+\n",
      "|  1|murali|  male|1000|null|\n",
      "|  2|geetha|female|null|  it|\n",
      "|  3|  abcd|  null|1000|  Hr|\n",
      "+---+------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \n",
    "    (1,'murali','male',1000,None),\n",
    "    (2,'geetha','female',None,'it'),\n",
    "    (3,'abcd',None,1000,'Hr')\n",
    "]\n",
    "\n",
    "schema = ('id','name','gender','dep')\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3466bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+----+-------+\n",
      "| id|  name| gender| dep|     _5|\n",
      "+---+------+-------+----+-------+\n",
      "|  1|murali|   male|1000|Unknown|\n",
      "|  2|geetha| female|null|     it|\n",
      "|  3|  abcd|Unknown|1000|     Hr|\n",
      "+---+------+-------+----+-------+\n",
      "\n",
      "+---+------+------+----+----+\n",
      "| id|  name|gender| dep|  _5|\n",
      "+---+------+------+----+----+\n",
      "|  1|murali|  male|1000|null|\n",
      "|  2|geetha|female|   0|  it|\n",
      "|  3|  abcd|  null|1000|  Hr|\n",
      "+---+------+------+----+----+\n",
      "\n",
      "+---+------+-------+----+----+\n",
      "| id|  name| gender| dep|  _5|\n",
      "+---+------+-------+----+----+\n",
      "|  1|murali|   male|1000|null|\n",
      "|  2|geetha| female|null|  it|\n",
      "|  3|  abcd|Unknown|1000|  Hr|\n",
      "+---+------+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna('Unknown').show() # this fill all text fields with specified  value 'Unknown' in data frame\n",
    "df.fillna(0).show() # since it is numaric values we are asking to fill where there are nulls, it is attempting to fill in numaric fields\n",
    "df.na.fill('Unknown',['gender','dep']).show() # this replaces the null value with 'Unknown' in gender and dep fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925552f",
   "metadata": {},
   "source": [
    "# sample()\n",
    "\n",
    "to get the random sampling subset from the large dataset\n",
    "\n",
    "Use function to indicate what percentage of data to return and seed value to make sure every time to get same random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f84688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 36|\n",
      "| 39|\n",
      "| 42|\n",
      "| 46|\n",
      "| 72|\n",
      "| 85|\n",
      "| 88|\n",
      "|100|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 36|\n",
      "| 39|\n",
      "| 42|\n",
      "| 46|\n",
      "| 72|\n",
      "| 85|\n",
      "| 88|\n",
      "|100|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This allows us to get sample from a data frame for profiling. \n",
    "#if you provide seed which gives same data set as a sample always\n",
    "df = spark.range(start = 1, end = 101)\n",
    "df1 = df.sample(fraction=0.1,seed = 123)\n",
    "df2 = df.sample(fraction=0.1,seed=123)\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2416c00",
   "metadata": {},
   "source": [
    "\n",
    "# Transform()\n",
    "\n",
    "it's is used ot chain the custom transformations and this function returns the new DataFrame after applying the specified transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c2a998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+---+\n",
      "| id|  name|gender|salary|dep|\n",
      "+---+------+------+------+---+\n",
      "|  1|murali|  male|  1000| IT|\n",
      "|  2|geetha|female|  2000| it|\n",
      "|  3|Chintu|  male|  1000| Hr|\n",
      "+---+------+------+------+---+\n",
      "\n",
      "+---+------+------+------+---+\n",
      "| id|  name|gender|salary|dep|\n",
      "+---+------+------+------+---+\n",
      "|  1|MURALI|  male|  1000| IT|\n",
      "|  2|GEETHA|female|  2000| it|\n",
      "|  3|CHINTU|  male|  1000| Hr|\n",
      "+---+------+------+------+---+\n",
      "\n",
      "+---+------+------+------+---+\n",
      "| id|  name|gender|salary|dep|\n",
      "+---+------+------+------+---+\n",
      "|  1|murali|  male|  2000| IT|\n",
      "|  2|geetha|female|  4000| it|\n",
      "|  3|Chintu|  male|  2000| Hr|\n",
      "+---+------+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "data = [\n",
    "    \n",
    "    (1,'murali','male',1000,'IT'),\n",
    "    (2,'geetha','female',2000,'it'),\n",
    "    (3,'Chintu','male',1000,'Hr')\n",
    "]\n",
    "\n",
    "schema = ('id','name','gender','salary','dep')\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "def convertToUpper(df):\n",
    "    return df.withColumn('name',upper(df.name))\n",
    "\n",
    "def doubleTheSal(df):\n",
    "    return df.withColumn('salary',df.salary * 2)\n",
    "\n",
    "df1 = df.transform(convertToUpper)\n",
    "df1.show()\n",
    "\n",
    "df1 = df.transform(doubleTheSal)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a3392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+---+\n",
      "| id|  name|gender|salary|dep|\n",
      "+---+------+------+------+---+\n",
      "|  1|murali|  male|  2000| IT|\n",
      "|  2|geetha|female|  4000| it|\n",
      "|  3|Chintu|  male|  2000| Hr|\n",
      "+---+------+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "043b1d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>murali</td>\n",
       "      <td>male</td>\n",
       "      <td>2000</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>geetha</td>\n",
       "      <td>female</td>\n",
       "      <td>4000</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Chintu</td>\n",
       "      <td>male</td>\n",
       "      <td>2000</td>\n",
       "      <td>Hr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name  gender  salary dep\n",
       "0   1  murali    male    2000  IT\n",
       "1   2  geetha  female    4000  it\n",
       "2   3  Chintu    male    2000  Hr"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cdd6a4",
   "metadata": {},
   "source": [
    "# createOrReplaceTempView()\n",
    "\n",
    "A temporary view is a way to create a pointer or reference to a DataFrame or Dataset that can be used to query the data using SQL-like syntax. The view is temporary and exists only for the duration of the Spark session, and can be accessed from any SparkSession connected to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b11db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "\n",
      "+------+----------+--------+\n",
      "|DEPTNO|     DNAME|LOCATION|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_path = '/home/murali/Desktop/data_files/emp.csv'\n",
    "\n",
    "emp = spark.read.csv(emp_path,header=True, inferSchema=True)\n",
    "emp.show()\n",
    "#emp.printSchema()\n",
    "\n",
    "dept_path = '/home/murali/Desktop/data_files/dept.csv'\n",
    "dept = spark.read.csv(dept_path,header=True, inferSchema=True)\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202c1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.createOrReplaceTempView('employees')\n",
    "dept.createOrReplaceTempView('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebff490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|      JOB|Total_SAL|\n",
      "+---------+---------+\n",
      "|  ANALYST|     6000|\n",
      "|  MANAGER|     5425|\n",
      "|PRESIDENT|     5000|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.sql('select employees.JOB, \\\n",
    "                        sum(employees.SAL) as Total_SAL \\\n",
    "                FROM employees WHERE employees.DEPTNO IN (10,20) \\\n",
    "                GROUP BY employees.JOB HAVING sum(employees.sal) >= 5000 \\\n",
    "                ')\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e825c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|      JOB|     DNAME|salary|\n",
      "+---------+----------+------+\n",
      "|  ANALYST|  RESEARCH|  6000|\n",
      "|PRESIDENT|ACCOUNTING|  5000|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept.createOrReplaceTempView('department')\n",
    "\n",
    "spark.sql('select e.JOB,d.DNAME , sum(e.sal) as salary \\\n",
    "FROM employees e left join \\\n",
    "department d \\\n",
    "on e.deptno = d.deptno \\\n",
    "WHERE e.DEPTNO IN (10,20)\\\n",
    "group by e.JOB,d.DNAME \\\n",
    "having sum(e.sal) >= 5000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7879a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.createOrReplaceGlobalTempView('empGlobal')\n",
    "dept.createOrReplaceGlobalTempView('deptGlobal')\n",
    "\n",
    "# when you create a global temp view, we need to use global_tem.tableName to access the table\n",
    "# this global temp view allows us to access this table across the spark sessions\n",
    "# All global Temp views will be created under global_temp name space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cce944a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from global_temp.empGlobal').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43223e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='department', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='employees', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list tables from current session\n",
    "spark.catalog.listTables(spark.catalog.currentDatabase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3056b304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='deptglobal', database='global_temp', description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='empglobal', database='global_temp', description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='department', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='employees', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to veiw glocal tem tables\n",
    "spark.catalog.listTables('global_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49cb3122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to drop the temp view\n",
    "spark.catalog.dropGlobalTempView('empGlobal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dd5f7",
   "metadata": {},
   "source": [
    "# user Defined Functions UDFs\n",
    "\n",
    "These are similar to functions in sql. We define some sogic in functions and store them in Database and use them in queries\n",
    "\n",
    "Similar to that we can write our own custom logic in python function and register in with pyspark using udf() function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e722ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e03ee517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to have bonus of 5% for dept 10 , 10% for Dep 20 and 15% for dep 30\n",
    "# if dept == 10 sal*(5/100) , if dept == 20 sal*(10/100) etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "080fbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "def totalBonus(s,d):\n",
    "    if s == 10:\n",
    "        bpct = s*0.05\n",
    "    elif s == 20:\n",
    "        bpct = s* 0.1\n",
    "    else:\n",
    "        bpct = s* 0.15\n",
    "    \n",
    "    return bpct\n",
    "\n",
    "\n",
    "bonus = udf(lambda s,d:totalBonus(s,d),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3617cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+----------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|totalBonus|\n",
      "+-----+------+---------+----+-----------+----+----+------+----------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|     120.0|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|     240.0|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|     187.5|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|    446.25|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|     187.5|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|     427.5|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|     367.5|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|     450.0|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|     750.0|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|     225.0|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|     165.0|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|     142.5|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|     450.0|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|     195.0|\n",
      "+-----+------+---------+----+-----------+----+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using this we are calculating bonus for each employee based on their department and salary\n",
    "# for dept 10 it is 5%, for Dept 20 it is 10% and for Dept 30 it is 15%\n",
    "emp_bonus = emp.withColumn('totalBonus',bonus(emp.SAL,emp.DEPTNO))\n",
    "emp_bonus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30f8af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can register the function to UDF at the time of creation in the follwing way\n",
    "\n",
    "@udf(returnType=FloatType())\n",
    "def totalBonus(s,d):\n",
    "    if s == 10:\n",
    "        bpct = s*0.05\n",
    "    elif s == 20:\n",
    "        bpct = s* 0.1\n",
    "    else:\n",
    "        bpct = s* 0.15\n",
    "    \n",
    "    return bpct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75dd9db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO| Bonus|\n",
      "+-----+------+---------+----+-----------+----+----+------+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20| 120.0|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30| 240.0|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30| 187.5|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|446.25|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30| 187.5|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30| 427.5|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10| 367.5|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20| 450.0|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10| 750.0|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30| 225.0|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20| 165.0|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30| 142.5|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20| 450.0|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10| 195.0|\n",
      "+-----+------+---------+----+-----------+----+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select('*',totalBonus(emp.SAL,emp.DEPTNO).alias(\"Bonus\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a67c4a",
   "metadata": {},
   "source": [
    "# RDD to dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d8eb825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'murali'), (2, 'wafa')]\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'murali'),(2,'wafa')]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03943bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd.toDF(schema = ['id','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e11780b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|murali|\n",
      "|  2|  wafa|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d87df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|murali|\n",
      "|  2|  wafa|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(rdd,schema = ['id','name'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfdfcc",
   "metadata": {},
   "source": [
    "# map()\n",
    "\n",
    "its RDD tranformation used to apply function(lambda) on every element of RDD and returns new RDD\n",
    "\n",
    "DataFrame don't support map() transformation to use with dataframe we need to generate RDD frist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f827374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [('reddy','pujari'),('geetha','tiruvaipati')]\n",
    "rdd = spark.sparkContext.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50a8e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('reddy', 'pujari', 'reddy pujari'), ('geetha', 'tiruvaipati', 'geetha tiruvaipati')]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda x:x+(x[0]+ ' ' +x[1],))\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "711465fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f16fac",
   "metadata": {},
   "source": [
    "# partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "834ecdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+----------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|totalBonus|\n",
      "+-----+------+---------+----+-----------+----+----+------+----------+\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|     120.0|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|     240.0|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|     187.5|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|    446.25|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|     187.5|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|     427.5|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|     367.5|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|     450.0|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|     750.0|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|     225.0|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|     165.0|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|     142.5|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|     450.0|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|     195.0|\n",
      "+-----+------+---------+----+-----------+----+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_bonus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8f928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83246e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'partitions/'\n",
    "\n",
    "#help(emp.write.parquet)\n",
    "emp.write.parquet(path=path,mode='overwrite',partitionBy='DEPTNO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa44a555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str\n",
      "        the path in any Hadoop supported file system\n",
      "    mode : str, optional\n",
      "        specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      "            exists.\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "        in the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(emp.write.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f136a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|\n",
      "+-----+------+---------+----+-----------+----+----+\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|\n",
      "+-----+------+---------+----+-----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('partitions/DEPTNO=10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b72c71b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "|EMPNO| ENAME|      JOB| MGR|   HIREDATE| SAL|COMM|DEPTNO|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "| 7499| ALLEN| SALESMAN|7698|20-FEB-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-SEP-1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|01-MAY-1981|2850|null|    30|\n",
      "| 7844|TURNER| SALESMAN|7698|08-SEP-1981|1500|   0|    30|\n",
      "| 7900| JAMES|    CLERK|7698|03-DEC-1981| 950|null|    30|\n",
      "| 7369| SMITH|    CLERK|7902|17-DEC-1980| 800|null|    20|\n",
      "| 7566| JONES|  MANAGER|7839|02-APR-1981|2975|null|    20|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-APR-1987|3000|null|    20|\n",
      "| 7876| ADAMS|    CLERK|7788|23-MAY-1987|1100|null|    20|\n",
      "| 7902|  FORD|  ANALYST|7566|03-DEC-1981|3000|null|    20|\n",
      "| 7782| CLARK|  MANAGER|7839|09-JUN-1981|2450|null|    10|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('partitions/').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a075d0",
   "metadata": {},
   "source": [
    "# from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b663973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------+\n",
      "|id    |props                            |\n",
      "+------+---------------------------------+\n",
      "|murali|{'hair' :'black', 'eye':'brown' }|\n",
      "|geetha|{'hair' :'brown', 'eye':'blue' } |\n",
      "+------+---------------------------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = (('murali',\"{'hair' :'black', 'eye':'brown' }\"), ('geetha',\"{'hair' :'brown', 'eye':'blue' }\"))\n",
    "\n",
    "schema = ['id','props']\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c4eccc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------+--------------+\n",
      "|id    |props                            |propsSturct   |\n",
      "+------+---------------------------------+--------------+\n",
      "|murali|{'hair' :'black', 'eye':'brown' }|{black, brown}|\n",
      "|geetha|{'hair' :'brown', 'eye':'blue' } |{brown, blue} |\n",
      "+------+---------------------------------+--------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      " |-- propsSturct: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "structTypeSchema = StructType([\\\n",
    "    StructField('hair',StringType()),\\\n",
    "    StructField('eye',StringType()) \\\n",
    "    ] )\n",
    "\n",
    "df1 = df.withColumn('propsSturct', from_json(df.props,structTypeSchema))\n",
    "df1.show(truncate=False)\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3bf4517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------+-----+-----+\n",
      "|    id|               props|   propsSturct| hair|  eye|\n",
      "+------+--------------------+--------------+-----+-----+\n",
      "|murali|{'hair' :'black',...|{black, brown}|black|brown|\n",
      "|geetha|{'hair' :'brown',...| {brown, blue}|brown| blue|\n",
      "+------+--------------------+--------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('hair',df1.propsSturct.hair)\\\n",
    ".withColumn('eye',df1.propsSturct.eye).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9f025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
