{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c516bd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/02 15:12:32 WARN Utils: Your hostname, murali-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/04/02 15:12:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/02 15:12:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/murali/spark-3.3.2-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MapType\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e134c",
   "metadata": {},
   "source": [
    "# A Map data type is nothing but a Dictonay type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7fef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  name|          properties|\n",
      "+------+--------------------+\n",
      "|Murali|{mother -> Padma,...|\n",
      "|Geetha|{mother -> Kastur...|\n",
      "|Kishan|{mother -> Geetha...|\n",
      "|  Sasi|{mother -> Geetha...|\n",
      "+------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [    \n",
    "    ('Murali',{'mother' : 'Padma', 'father' : 'venkat' }),\n",
    "    ('Geetha',{'mother' : 'Kasturi', 'father' : 'Varada' }),\n",
    "    ('Kishan',{'mother' : 'Geetha', 'father' : 'Murali' }),\n",
    "    ('Sasi',{'mother' : 'Geetha', 'father' : 'Murali' })\n",
    "]\n",
    "\n",
    "schema = ['name','properties']\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69e2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "schema = StructType([\n",
    "          StructField('Name', dataType=StringType()),\n",
    "          StructField('Parents', dataType=MapType(keyType=StringType(), valueType=StringType()))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de1e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema= schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631b0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  Name|             Parents|\n",
      "+------+--------------------+\n",
      "|Murali|{mother -> Padma,...|\n",
      "|Geetha|{mother -> Kastur...|\n",
      "|Kishan|{mother -> Geetha...|\n",
      "|  Sasi|{mother -> Geetha...|\n",
      "+------+--------------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Parents: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51522544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------+------+-------+\n",
      "|Name  |Parents                              |father|mother |\n",
      "+------+-------------------------------------+------+-------+\n",
      "|Murali|{mother -> Padma, father -> venkat}  |venkat|Padma  |\n",
      "|Geetha|{mother -> Kasturi, father -> Varada}|Varada|Kasturi|\n",
      "|Kishan|{mother -> Geetha, father -> Murali} |Murali|Geetha |\n",
      "|Sasi  |{mother -> Geetha, father -> Murali} |Murali|Geetha |\n",
      "+------+-------------------------------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('father',df.Parents['father']).\\\n",
    "withColumn('mother',df.Parents['mother']).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02119063",
   "metadata": {},
   "source": [
    "# getItem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6f846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------+------+\n",
      "|Name  |Parents                              |father|\n",
      "+------+-------------------------------------+------+\n",
      "|Murali|{mother -> Padma, father -> venkat}  |venkat|\n",
      "|Geetha|{mother -> Kasturi, father -> Varada}|Varada|\n",
      "|Kishan|{mother -> Geetha, father -> Murali} |Murali|\n",
      "|Sasi  |{mother -> Geetha, father -> Murali} |Murali|\n",
      "+------+-------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('father', df.Parents.getItem('father')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaac0f5",
   "metadata": {},
   "source": [
    "# using spark sql - createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297595c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------+\n",
      "|Name  |Parents                              |\n",
      "+------+-------------------------------------+\n",
      "|Murali|{mother -> Padma, father -> venkat}  |\n",
      "|Geetha|{mother -> Kasturi, father -> Varada}|\n",
      "|Kishan|{mother -> Geetha, father -> Murali} |\n",
      "|Sasi  |{mother -> Geetha, father -> Murali} |\n",
      "+------+-------------------------------------+\n",
      "\n",
      "+------+------+-------+\n",
      "|  name|Father| mother|\n",
      "+------+------+-------+\n",
      "|Murali|venkat|  Padma|\n",
      "|Geetha|Varada|Kasturi|\n",
      "|Kishan|Murali| Geetha|\n",
      "|  Sasi|Murali| Geetha|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('family')\n",
    "results = spark.sql('select * from family')\n",
    "results.show(truncate=False)\n",
    "\n",
    "spark.sql('select name, split(Parents[\\'father\\'],\":\")[0] as Father, \\\n",
    "          split(Parents[\\'mother\\'],\":\")[0] as mother from family').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e1d39",
   "metadata": {},
   "source": [
    "# explode(), map_keys() & map_values()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c9937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  Name|             Parents|\n",
      "+------+--------------------+\n",
      "|Murali|{mother -> Padma,...|\n",
      "|Geetha|{mother -> Kastur...|\n",
      "|Kishan|{mother -> Geetha...|\n",
      "|  Sasi|{mother -> Geetha...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7349484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------+-------+------+\n",
      "|Name  |Parents                              |mother |father|\n",
      "+------+-------------------------------------+-------+------+\n",
      "|Murali|{mother -> Padma, father -> venkat}  |Padma  |venkat|\n",
      "|Geetha|{mother -> Kasturi, father -> Varada}|Kasturi|Varada|\n",
      "|Kishan|{mother -> Geetha, father -> Murali} |Geetha |Murali|\n",
      "|Sasi  |{mother -> Geetha, father -> Murali} |Geetha |Murali|\n",
      "+------+-------------------------------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values, map_keys\n",
    "\n",
    "df.withColumn('mother',map_values(df.Parents)[0]).\\\n",
    "withColumn('father',map_values(df.Parents)[1]).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b35259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------------+\n",
      "|  Name|             Parents|            Keys|\n",
      "+------+--------------------+----------------+\n",
      "|Murali|{mother -> Padma,...|[mother, father]|\n",
      "|Geetha|{mother -> Kastur...|[mother, father]|\n",
      "|Kishan|{mother -> Geetha...|[mother, father]|\n",
      "|  Sasi|{mother -> Geetha...|[mother, father]|\n",
      "+------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('Keys',map_keys(df.Parents)) \n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc32362",
   "metadata": {},
   "source": [
    "# Row Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ff1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1565c30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n",
      "\n",
      "class Row(builtins.tuple)\n",
      " |  Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      " |  \n",
      " |  A row in :class:`DataFrame`.\n",
      " |  The fields in it can be accessed:\n",
      " |  \n",
      " |  * like attributes (``row.key``)\n",
      " |  * like dictionary values (``row[key]``)\n",
      " |  \n",
      " |  ``key in row`` will search through row keys.\n",
      " |  \n",
      " |  Row can be used to create a row object by using named arguments.\n",
      " |  It is not allowed to omit a named argument to represent that the value is\n",
      " |  None or missing. This should be explicitly set to None in this case.\n",
      " |  \n",
      " |  .. versionchanged:: 3.0.0\n",
      " |      Rows created from named arguments no longer have\n",
      " |      field names sorted alphabetically and will be ordered in the position as\n",
      " |      entered.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> row = Row(name=\"Alice\", age=11)\n",
      " |  >>> row\n",
      " |  Row(name='Alice', age=11)\n",
      " |  >>> row['name'], row['age']\n",
      " |  ('Alice', 11)\n",
      " |  >>> row.name, row.age\n",
      " |  ('Alice', 11)\n",
      " |  >>> 'name' in row\n",
      " |  True\n",
      " |  >>> 'wrong_key' in row\n",
      " |  False\n",
      " |  \n",
      " |  Row also can be used to create another Row like class, then it\n",
      " |  could be used to create Row objects, such as\n",
      " |  \n",
      " |  >>> Person = Row(\"name\", \"age\")\n",
      " |  >>> Person\n",
      " |  <Row('name', 'age')>\n",
      " |  >>> 'name' in Person\n",
      " |  True\n",
      " |  >>> 'wrong_key' in Person\n",
      " |  False\n",
      " |  >>> Person(\"Alice\", 11)\n",
      " |  Row(name='Alice', age=11)\n",
      " |  \n",
      " |  This form can also be used to create rows as tuple values, i.e. with unnamed\n",
      " |  fields.\n",
      " |  \n",
      " |  >>> row1 = Row(\"Alice\", 11)\n",
      " |  >>> row2 = Row(name=\"Alice\", age=11)\n",
      " |  >>> row1 == row2\n",
      " |  True\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Row\n",
      " |      builtins.tuple\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args: Any) -> 'Row'\n",
      " |      create new Row object\n",
      " |  \n",
      " |  __contains__(self, item: Any) -> bool\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattr__(self, item: str) -> Any\n",
      " |  \n",
      " |  __getitem__(self, item: Any) -> Any\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __reduce__(self) -> Union[str, Tuple[Any, ...]]\n",
      " |      Returns a tuple so Python knows how to pickle Row.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Printable representation of Row used in Python REPL.\n",
      " |  \n",
      " |  __setattr__(self, key: Any, value: Any) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  asDict(self, recursive: bool = False) -> Dict[str, Any]\n",
      " |      Return as a dict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      recursive : bool, optional\n",
      " |          turns the nested Rows to dict (default: False).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If a row contains duplicate field names, e.g., the rows of a join\n",
      " |      between two :class:`DataFrame` that both have the fields of same names,\n",
      " |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n",
      " |      will also return one of the duplicate fields, however returned value might\n",
      " |      be different to ``asDict``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      " |      True\n",
      " |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      " |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n",
      " |      True\n",
      " |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getnewargs__(self, /)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  count(self, value, /)\n",
      " |      Return number of occurrences of value.\n",
      " |  \n",
      " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      " |      Return first index of value.\n",
      " |      \n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __class_getitem__(...) from builtins.type\n",
      " |      See PEP 585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd7c402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = Row('Murali',200)\n",
    "type(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280f14d",
   "metadata": {},
   "source": [
    "# a Row type creats a tuple so to access this we need to use indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c8eedf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murali\n"
     ]
    }
   ],
   "source": [
    "print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e77eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murali\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "row = Row(name = 'Murali', salary = 200)\n",
    "print(row.name)\n",
    "print(row.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "689b78df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "murali\n",
      "geetha\n"
     ]
    }
   ],
   "source": [
    "# Row asts as class as well\n",
    "\n",
    "row1 = Row('name', 'salary') # This one is creaing a row type class\n",
    "p1 = row1('murali',2000) # Now in p1 we are storing data that is row1 type\n",
    "p2 = row1('geetha',300)\n",
    "print(p1.name)\n",
    "print(p2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716a0b5",
   "metadata": {},
   "source": [
    "# Column Class in pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982d00d",
   "metadata": {},
   "source": [
    "pyspark Column class represents a single column in a DataFrame\n",
    "\n",
    "pyspark.sql.Column class provides several functions to work with\n",
    "DataFram to manipulate the Column values, evaluate the boolean expression to filer rows, \n",
    "retrieve a value or part of a value from a DataFram column\n",
    "\n",
    "One of the simplest way to create a Column class is by using Pyspark lit() Sql function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45f38698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "col1 = lit(\"abcd\")\n",
    "print(type(col1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22a7ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,StructField, IntegerType,StringType\n",
    "\n",
    "data = [    \n",
    "    ('Murali','male',2000),\n",
    "    ('Geetha','female',3000),\n",
    "    ('Kishan', 'male',4000),\n",
    "    ('Sasi', 'male',5000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef474fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('name', dataType=StringType()),\n",
    "    StructField('gender', dataType=StringType()),\n",
    "    StructField('salary', dataType=StringType())\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdc0bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|  name|gender|salary|\n",
      "+------+------+------+\n",
      "|Murali|  male|  2000|\n",
      "|Geetha|female|  3000|\n",
      "|Kishan|  male|  4000|\n",
      "|  Sasi|  male|  5000|\n",
      "+------+------+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#schema = ['name','gender','salary']\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "099bc1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+\n",
      "|  name|gender|salary|   newCol|\n",
      "+------+------+------+---------+\n",
      "|Murali|  male|  2000|newColVal|\n",
      "|Geetha|female|  3000|newColVal|\n",
      "|Kishan|  male|  4000|newColVal|\n",
      "|  Sasi|  male|  5000|newColVal|\n",
      "+------+------+------+---------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- newCol: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('newCol', lit('newColVal'))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53776a66",
   "metadata": {},
   "source": [
    "Accessing columns in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5c41677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Murali|\n",
      "|Geetha|\n",
      "|Kishan|\n",
      "|  Sasi|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df1.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f684c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Murali|\n",
      "|Geetha|\n",
      "|Kishan|\n",
      "|  Sasi|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1883d112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Murali|\n",
      "|Geetha|\n",
      "|Kishan|\n",
      "|  Sasi|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5ab6064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Murali|\n",
      "|Geetha|\n",
      "|Kishan|\n",
      "|  Sasi|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df85cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [    \n",
    "    ('Murali','male',2000,('pySpark','snowflake')),\n",
    "    ('Geetha','female',3000,('Unix','webx')),\n",
    "    ('Kishan', 'male',4000,('Data Sciences','ML')),\n",
    "    ('Sasi', 'male',5000,('python','AWS'))\n",
    "]\n",
    "\n",
    "skills_Stru = StructType(\n",
    "[\n",
    "    StructField('Skill1', dataType=StringType()),\n",
    "    StructField('Skill2', dataType=StringType())\n",
    "]\n",
    ")     \n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', dataType=StringType()),\n",
    "    StructField('gender', dataType=StringType()),\n",
    "    StructField('salary', dataType=StringType()),\n",
    "    StructField('skills', dataType=skills_Stru)\n",
    "]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98db758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+--------------------+\n",
      "|name  |gender|salary|skills              |\n",
      "+------+------+------+--------------------+\n",
      "|Murali|male  |2000  |{pySpark, snowflake}|\n",
      "|Geetha|female|3000  |{Unix, webx}        |\n",
      "|Kishan|male  |4000  |{Data Sciences, ML} |\n",
      "|Sasi  |male  |5000  |{python, AWS}       |\n",
      "+------+------+------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- skills: struct (nullable = true)\n",
      " |    |-- Skill1: string (nullable = true)\n",
      " |    |-- Skill2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3051cfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+-------------+\n",
      "|  name|gender|skills.Skill1|skills.Skill2|\n",
      "+------+------+-------------+-------------+\n",
      "|Murali|  male|      pySpark|    snowflake|\n",
      "|Geetha|female|         Unix|         webx|\n",
      "|Kishan|  male|Data Sciences|           ML|\n",
      "|  Sasi|  male|       python|          AWS|\n",
      "+------+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'],df['gender'],df['skills'].Skill1, df['skills'].Skill2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f40ea291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+-------------+\n",
      "|  name|gender|skills.skill1|skills.skill2|\n",
      "+------+------+-------------+-------------+\n",
      "|Murali|  male|      pySpark|    snowflake|\n",
      "|Geetha|female|         Unix|         webx|\n",
      "|Kishan|  male|Data Sciences|           ML|\n",
      "|  Sasi|  male|       python|          AWS|\n",
      "+------+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,df.gender,df.skills.skill1,df.skills.skill2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "deddb9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|  name|       skill1|\n",
      "+------+-------------+\n",
      "|Murali|      pySpark|\n",
      "|Geetha|         Unix|\n",
      "|Kishan|Data Sciences|\n",
      "|  Sasi|       python|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('name'),col('skills.skill1')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50786b8c",
   "metadata": {},
   "source": [
    "# when() otehrwise() \n",
    "###  These are just like case statements in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8b132a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|Kishan|      |  4000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,'Murali','male',2000),\n",
    "    (2,'Geetha','female',3000),\n",
    "    (3,'Kishan', '',4000),\n",
    "    (4,'Sasi', 'male',5000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', dataType=IntegerType()),\n",
    "    StructField('name', dataType=StringType()),\n",
    "    StructField('gender', dataType=StringType()),\n",
    "    StructField('salary', dataType=IntegerType())\n",
    "    \n",
    "]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c425b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1bb684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|f_m|\n",
      "+---+------+---+\n",
      "|  1|Murali|  m|\n",
      "|  2|Geetha|  f|\n",
      "|  3|Kishan|  m|\n",
      "|  4|  Sasi|  m|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(\n",
    "        df.id,\n",
    "        df.name,\n",
    "        when(df.gender=='female','f')\\\n",
    "            .when(df.gender == 'male','m')\\\n",
    "            .otherwise('m').alias('f_m')\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63bee651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function when in module pyspark.sql.functions:\n",
      "\n",
      "when(condition: pyspark.sql.column.Column, value: Any) -> pyspark.sql.column.Column\n",
      "    Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "    If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "    conditions.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    condition : :class:`~pyspark.sql.Column`\n",
      "        a boolean :class:`~pyspark.sql.Column` expression.\n",
      "    value :\n",
      "        a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
      "    [Row(age=3), Row(age=4)]\n",
      "    \n",
      "    >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
      "    [Row(age=3), Row(age=None)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(when)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386e2b1",
   "metadata": {},
   "source": [
    "# Funcitions to apply on top of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50797191",
   "metadata": {},
   "source": [
    "### alias(), sort(),asc(), desc(), cast(), like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5cf6966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|Kishan|      |  4000|\n",
      "|  1|Murali|  male|  2000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.name.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "089178ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|   sal|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|2000.0|\n",
      "|  2|Geetha|female|3000.0|\n",
      "|  3|Kishan|      |4000.0|\n",
      "|  4|  Sasi|  male|5000.0|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id,df.name,df.gender,\n",
    "          df.salary.cast('float').alias('sal')\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "601331cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|   sal|\n",
      "+---+------+------+------+\n",
      "|  2|Geetha|female|3000.0|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id,df.name,df.gender,\\\n",
    "          df.salary.cast('float').alias('sal')\\\n",
    "         )\\\n",
    ".filter(df.gender.like('f%'))\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd43d55",
   "metadata": {},
   "source": [
    "# filter() & where() in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e8609c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  3|Kishan|      |  4000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.salary>3000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26799bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  2|Geetha|female|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.gender == 'female').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e37bcb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|Kishan|      |  4000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.id <= 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adc9cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where( (df.id<=3) & (df.gender == 'male') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "968948ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|Kishan|      |  4000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter( (df.gender.like('f%') ) | (df.id <= 3) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5348bb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  2|Geetha|female|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter( (df.gender.like('f%') ) & (df.id <= 3) ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ac15c",
   "metadata": {},
   "source": [
    "# distinct() & dropDuplicates()\n",
    "\n",
    "both work in same way but in case of dropDuplicates(), we can spcifiy columns as a list to consider to remove duplcates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3811fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  1|Murali|  male|  5000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|   abc|      |  4000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,'Murali','male',2000),\n",
    "    (1,'Murali','male',5000),\n",
    "    (2,'Geetha','female',3000),\n",
    "    (2,'Geetha','female',3000),\n",
    "    (3,'abc','',4000),\n",
    "    (3,'Kishan', 'male',4000),\n",
    "    (3,'Kishan', 'male',4000),\n",
    "    (4,'Sasi', 'male',5000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', dataType=IntegerType()),\n",
    "    StructField('name', dataType=StringType()),\n",
    "    StructField('gender', dataType=StringType()),\n",
    "    StructField('salary', dataType=IntegerType())\n",
    "    \n",
    "]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a91882d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  1|Murali|  male|  5000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|   abc|      |  4000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bde2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.gender != '').dropDuplicates(('id', 'name')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1bc6f",
   "metadata": {},
   "source": [
    "# orderBy() & sort() \n",
    "\n",
    "Both of them work in same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a23318cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  1|Murali|  male|  5000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|   abc|      |  4000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f91978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  2|Geetha|  3000|\n",
      "|  3|Kishan|  4000|\n",
      "|  1|Murali|  2000|\n",
      "|  4|  Sasi|  5000|\n",
      "|  3|   abc|  4000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc , asc\n",
    "df.dropDuplicates(['name', 'gender']) \\\n",
    ".select('id', 'name', 'salary') \\\n",
    "  .orderBy('name', desc('salary')) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8817cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8cfb22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  4|  Sasi|  5000|\n",
      "|  3|Kishan|  4000|\n",
      "|  3|   abc|  4000|\n",
      "|  2|Geetha|  3000|\n",
      "|  1|Murali|  2000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc, when, col\n",
    "# we need to replace null values or blabnks with some value for sort / orderby to work correctly\n",
    "df.dropDuplicates(['name', 'gender']) \\\n",
    "  .orderBy(when(col('gender') == '', 'a').otherwise('a'), desc('salary'), asc('name')) \\\n",
    "  .select('id', 'name', 'salary') \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87f159ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|Murali|  male|  2000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  2|Geetha|female|  3000|\n",
      "|  3|   abc|      |  4000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  3|Kishan|  male|  4000|\n",
      "|  4|  Sasi|  male|  5000|\n",
      "|  1|Murali|  male|  5000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.salary,df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb9a80",
   "metadata": {},
   "source": [
    "# Union & Union All\n",
    "\n",
    "union and unionAll transformations are used to merge two or mor DataFrames of the same schema or structure\n",
    "\n",
    "Unlike SQL both union and unionAll act similar way. they don't remove duplicates.\n",
    "\n",
    "To remove duplicates, we need to use distinct() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80d50821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, name: string, gender: string, salary: int, job: string]\n",
      "DataFrame[id: int, name: string, gender: string, salary: int, job: string]\n"
     ]
    }
   ],
   "source": [
    "mFamily = [\n",
    "        (1, 'Murali', 'M', 2000, 'IT'),\n",
    "        (2, 'Geetha', 'F', 3000, 'IT'),\n",
    "        (3, 'Chintu', 'M', 4000, 'IT'),\n",
    "        (4, 'Chinna', 'M', 5000, 'IT')    \n",
    "]\n",
    "\n",
    "eFamily = [\n",
    "        (1, 'Murali', 'M', 2000, 'IT'),\n",
    "        (2, 'Geetha', 'F', 3000, 'IT'),\n",
    "        (3, 'Chintu', 'M', 4000, 'IT'),\n",
    "        (4, 'Chinna', 'M', 5000, 'IT'),\n",
    "        (5, 'venkatramaiah', 'M', 6000, 'Lecturer'),\n",
    "        (6, 'Padmavatamma', 'F', 5000, 'Teacher'),\n",
    "        (7, 'Prasad', 'M', 7000, 'Business'),\n",
    "        (8, 'Sumitra', 'F', 6000, 'IT'),\n",
    "        (9, 'Poojita', 'F', 6000, 'Student')\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', dataType=IntegerType()),\n",
    "    StructField('name', dataType=StringType()),\n",
    "    StructField('gender', dataType=StringType()),\n",
    "    StructField('salary', dataType=IntegerType()),\n",
    "    StructField('job', dataType=StringType())\n",
    "    \n",
    "]\n",
    ")\n",
    "\n",
    "myFamily = spark.createDataFrame(mFamily,schema = schema)\n",
    "extendedFamily = spark.createDataFrame(eFamily,schema = schema)\n",
    "\n",
    "totalFamily = myFamily.union(extendedFamily)\n",
    "totalFamily2 = myFamily.unionAll(extendedFamily).distinct()\n",
    "\n",
    "print(totalFamily)\n",
    "print(totalFamily2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5244756d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalFamily2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992459e2",
   "metadata": {},
   "source": [
    "# groupBy()\n",
    "\n",
    "investigate on groupByName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdd8f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+-----------+\n",
      "|     job|gender|sum(id)|sum(salary)|\n",
      "+--------+------+-------+-----------+\n",
      "|Business|     M|      7|       7000|\n",
      "| Teacher|     F|      6|       5000|\n",
      "|Lecturer|     M|      5|       6000|\n",
      "|      IT|     M|      8|      11000|\n",
      "| Student|     F|      9|       6000|\n",
      "|      IT|     F|     10|       9000|\n",
      "+--------+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalFamily2.groupBy(['job','gender']).sum().show()\n",
    "\n",
    "# we are getting even id colum also getting summed which is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80d3b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|     job|gender|sum(salary)|\n",
      "+--------+------+-----------+\n",
      "|Business|     M|       7000|\n",
      "| Teacher|     F|       5000|\n",
      "|Lecturer|     M|       6000|\n",
      "|      IT|     M|      11000|\n",
      "| Student|     F|       6000|\n",
      "|      IT|     F|       9000|\n",
      "+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalFamily2.select(['job','gender','name','salary']).groupBy(['job','gender']).sum('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0da4e",
   "metadata": {},
   "source": [
    "# groupBy agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e649a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,min,max,sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ec38f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------------+-------------+\n",
      "|     job|gender|number of persons|sum of Salary|\n",
      "+--------+------+-----------------+-------------+\n",
      "|Business|     M|                1|         7000|\n",
      "| Teacher|     F|                1|         5000|\n",
      "|Lecturer|     M|                1|         6000|\n",
      "|      IT|     M|                3|        11000|\n",
      "| Student|     F|                1|         6000|\n",
      "|      IT|     F|                2|         9000|\n",
      "+--------+------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalFamily2                                \\\n",
    ".distinct()                                 \\\n",
    ".groupBy(['job','gender'])                  \\\n",
    ".agg(                                       \\\n",
    "    count('id').alias('number of persons')  \\\n",
    "    ,sum('salary').alias('sum of Salary')   \\\n",
    "    )                                       \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a7a57",
   "metadata": {},
   "source": [
    "# unionByName()\n",
    "\n",
    "This allows us to merge two dataframes even if they don't have matching columns.\n",
    "groupBy() will merge the DataFrames using their column indexs.\n",
    "Where as groupByName will merge them using the column names. but we need to specify alloMissingColumns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60df2015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|murali| 50|\n",
      "|  1|geetha| 49|\n",
      "+---+------+---+\n",
      "\n",
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|murali|  5000|\n",
      "|  1|geetha|  2000|\n",
      "+---+------+------+\n",
      "\n",
      "+---+------+----+------+\n",
      "| id|  name| age|salary|\n",
      "+---+------+----+------+\n",
      "|  1|murali|  50|  null|\n",
      "|  1|geetha|  49|  null|\n",
      "|  1|murali|null|  5000|\n",
      "|  1|geetha|null|  2000|\n",
      "+---+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = [\n",
    "    \n",
    "    (1,'murali',50),(1,'geetha',49)\n",
    "]\n",
    "s1 = StructType([\n",
    "    StructField('id',dataType = IntegerType()),\n",
    "    StructField('name',dataType = StringType()),\n",
    "    StructField('age',dataType = IntegerType())\n",
    "]\n",
    ")\n",
    "\n",
    "d2 = [\n",
    "    \n",
    "    (1,'murali',5000),(1,'geetha',2000)\n",
    "]\n",
    "s2 = StructType([\n",
    "    StructField('id',dataType = IntegerType()),\n",
    "    StructField('name',dataType = StringType()),\n",
    "    StructField('salary',dataType = IntegerType())\n",
    "]\n",
    ")\n",
    "\n",
    "df1 = spark.createDataFrame(d1,schema = s1)\n",
    "\n",
    "df2 = spark.createDataFrame(d2,schema=s2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "df1.unionByName(df2,allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20df38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
